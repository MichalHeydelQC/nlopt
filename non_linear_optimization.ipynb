{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Linear Optimazation (nlopt) short tutorial\n",
    "---\n",
    "\n",
    "Mission:  \n",
    "\n",
    "$\\min_{\\mathbf{x}\\in\\mathbb{R}^2} \\sqrt{x_2}$  \n",
    "subject to $x_2 \\geq 0, x_2 \\geq (a_1 x_1 + b_1)^3, x_2 \\geq (a_2 x_1 + b_2)^3 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"NLopt-example-constraints.png\" width=\"600\">\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[NLopt_Algorithms](https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlopt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myfunc(x, grad):\n",
    "    if grad.size > 0:\n",
    "        grad[0] = 0.0  # partial derivative of the objective function to x[0]\n",
    "        grad[1] = 0.5 / np.sqrt(x[1]) n# partial derivative of the objective function to x[1]\n",
    "    return np.sqrt(x[1]) # Objective function\n",
    "\n",
    "def myconstraint(x, grad, a, b):\n",
    "    if grad.size > 0:\n",
    "        grad[0] = 3 * a * (a*x[0] + b)**2  # derivative wrt x_0\n",
    "        grad[1] = -1.0  # derivative wrt x_1\n",
    "    return (a*x[0] + b)**3 - x[1]  # constraint: (a*x_0 + b)^3 - x_1 =<0\n",
    "\n",
    "# `grad` is only for gradient-based methods!!\n",
    "# if you a derivative-free optimazation algorithm, `grad` will always be `NULL` and you never need to compute any derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the optimizer:\n",
    "# Notation used in NLOPT: NLOPT_{G,L}{N,D}_xxxx, where G/L denotes global/local optimization -\n",
    "# and N/D denotes derivative-free/gradient-based algorithms, respectively.\n",
    "# Examples:\n",
    "# \"LD\" means local optimization, derivative/gradient-based\n",
    "# \"LN\" means local optimization, no derivatives\n",
    "\n",
    "# NLOPT_AUGLAG - Augmented Lagrangian.\n",
    "opt = nlopt.opt(nlopt.LD_MMA, 2)  # Method of Moving Asymptotes\n",
    "\n",
    "opt.set_lower_bounds([-float('inf'), 0])\n",
    "opt.set_min_objective(myfunc)\n",
    "\n",
    "opt.add_inequality_constraint(lambda x, grad: myconstraint(x,grad, 2, 0), 1e-8)  # 1e-8 is a tolerance for the constraint!\n",
    "opt.add_inequality_constraint(lambda x, grad: myconstraint(x,grad, -1, 1), 1e-8)\n",
    "opt.set_xtol_rel(1e-4) # tolerance for x parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimum at  0.33333356358469857 0.29629601033128866\n",
      "minimum value =  0.5443307912761216\n",
      "result code =  4\n"
     ]
    }
   ],
   "source": [
    "x = opt.optimize([np.random.uniform(low=0, high=1100), np.random.uniform(low=0, high=1100)]) # starting values\n",
    "minf = opt.last_optimum_value()\n",
    "print(\"optimum at \", x[0], x[1])\n",
    "print(\"minimum value = \", minf)\n",
    "print(\"result code = \", opt.last_optimize_result())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
